createdAt: "2021-06-17T07:19:02.501Z"
updatedAt: "2021-06-17T07:21:29.139Z"
type: "MARKDOWN_NOTE"
folder: "0a10886e53840528eb15"
title: "代码tricks"
tags: []
content: '''
  # 代码tricks
  
  
  1. 验证加的某个模块有没有效果，对性能有没有提升的时候如果训练集太大，可以在一个小数据集上进行实验
  2. 模型的不同层可以用不同的learining rate
  
  ~~~python
      # ============================ step 4/5 优化器 ============================
      flag = 0
      # flag = 1
      if flag:
          fc_params_id = list(map(id, alexnet_model.classifier.parameters()))  # 返回的是parameters的 内存地址
          base_params = filter(lambda p: id(p) not in fc_params_id, alexnet_model.parameters())
          optimizer = optim.SGD([
              {'params': base_params, 'lr': LR * 0.1},  # 0
              {'params': alexnet_model.classifier.parameters(), 'lr': LR}], momentum=0.9)
  
      else:
          optimizer = optim.SGD(alexnet_model.parameters(), lr=LR, momentum=0.9)  # 选择优化器
  
      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)  # 设置学习率下降策略
      # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(patience=5)
  ~~~
'''
linesHighlighted: []
isStarred: false
isTrashed: false
