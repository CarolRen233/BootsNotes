createdAt: "2019-11-18T07:09:14.320Z"
updatedAt: "2019-11-18T11:07:03.961Z"
type: "MARKDOWN_NOTE"
folder: "f1fd0a362703e39f5960"
title: "Meta-learning Survey"
tags: []
content: '''
  # Meta-learning Survey
  
  [TOC]
  
  [论文地址](https://arxiv.org/pdf/1810.03548.pdf)
  
  ## 1. Motivation
  
  人类是可以利用之前相关的一些task中学到的东西，应用到新的task中的。我们希望机器也可以做到这一点，那么就需要
  
  
  1. **meta-data**： algorithm configurations、model evaluations、meta-features.
  2. **learn from this prior meta-data**
  
  我们认为meta-learning就是之前的task和新的task月similar，我们就越能leverage 
  
  ## 2. Classification
  
  ### 2.1 Learning from Model Evaluations
  
  >model evaluations, such as accuracy and training time, the learned model parameters, such as the trained weights of a neural net, as well as measurable properties of the task itself, also known as meta-features
  
  #### 2.1.1 Task-Independent Recommendations
  
  这个方法就是排行榜??看哪个模型的效果更好
  
  #### 2.1.2 Configuration Space Design
  
  超参数在这里很重要
  
  找到defult的表现一直都很好的超参数的值
  
  #### 2.1.3 Configuration Transfer
  
  >P is the set of all priorscalar evaluations
  
  就是看task的相似程度(就是用相同的configurations，然后看P的值，P如果相似，那么就认为这两个task是相似的)
  
  
  ## 2.2 Learning from Task Properties
  
  >task itself, also known as meta-features.
  >$$
  >\\begin{aligned}
  >&task: t_{j}\\in T \\\\
  >&vector \\  m(t_{j})=(m_{j,1}...m
  _{k,K})\\ of \\ K \\ meta-features
  >\\end{aligned}
  >$$
  >
  >meta-features 有这些：
  >![b035255f.png](:storage\\8a2736b3-41db-47f0-ad8e-d7aef3e8d452\\b035255f.png)
  
  每个task用向量来表示，然后衡量两个向量的相似程度来看他们是不是similar的task
  
  
  
  ## 2.3 Learning from Prior Models
  
  ### 2.3.1 Transfer Learning
  
  >we take models trained on one or more source tasks tj , and use them as starting points for creating a model on a similar target task tnew.
  >This can be done by forcing the target model to be structurally or otherwise similar to the source model(s).
  >
  
  transfer learning的方法有
  
  - kernel methods 
  - parametric Bayesian models 
  - Bayesian networks
  - clustering
  - reinforcement learning 
  
  神经网络的方法尤其适合transfer learning，因为structure和model parameter都以作为很好的initialization，就是在t_{new}新的task中，对pre-trained model进行finetune
   
  ### 2.3.2 Meta-Learning in Neural Networks
  
  **早期的meta-learning使用的是RNN,就是将weights也作为一inpot data，一同去学习error，然后modify these weights**
  
  **后来的方法就是采用reinforcement learning across tasks to adapt the search strategy**
  
  **还有一些LSTM等的方法**
  
  ### 2.3.3 Few-Shot Learning
  
  >A particularly challenging meta-learning problem is to train an accurate deep learning model using only a few training examples, given prior experience with very similar tasks for which we have large training sets available
  >
  
  **早期**
  
  早期的研究都是hand-engineered features，例如李飞飞2006的一些研究
  
  **后来**
  
  1. [Matching networks for one shot learning](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf)这篇论文(引用量976)，认为需要non parameteric models，例如k-nearest neighbors， **matches each new test instance to the memorized examples using cosine similarity.**
  2. [Prototypical networks for few-shot learning](http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf)的Prototypical Networks(引用量666)把examples map到一个p维向量空间，然后使那些在同一个class的离得更近。然后计算每个class的prototype (mean vector)，对于新的test instance就也map到这个空间，然后计算softmax over all possible classes. [Meta-learning for semi-supervised few-shot classification](https://arxiv.org/pdf/1803.00676.pdf)在这个基础上将其改成半监督的
  3. [Optimization as a model for few-shot learning](https://openreview.net/pdf?id=rJY0-Kcll)(引用量530)用了LSTM的方法，每次有个新的example进来，就计算loss，用梯度下降来更新parameters
  4. [Model-agnostic meta-learning for fast adaptation of deep networks](https://arxiv.org/pdf/1703.03400.pdf)(引用量999)MAML,用一个随机的initial的parameter值w_{k}，选择一组prior tasks，然后开始用J examples取train，计算梯度和loss，然后更新w_{k}。[Meta-learning and universality](https://arxiv.org/pdf/1710.11622.pdf)证明了这种方法的universality
  5. [Meta-learning with memory-augmented neural networks.](http://proceedings.mlr.press/v48/santoro16.pdf)(引用量408)利用了[Neural turing machines](https://arxiv.org/pdf/1410.5401.pdf%20(http://Neural%20Turning%20Machines)%20)(引用量1202)NTMs这个网络，是一种有记忆能力的网络
  
  
  
'''
linesHighlighted: []
isStarred: false
isTrashed: false
